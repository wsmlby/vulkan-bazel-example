#version 450
#extension GL_KHR_cooperative_matrix : require
#extension GL_KHR_memory_scope_semantics : require
#extension GL_KHR_shader_subgroup_basic : require
#extension GL_EXT_shader_explicit_arithmetic_types_float16 : require

// Fused Conv2D + SiLU using cooperative matrix (tensor core) acceleration
// Mixed precision: fp16 A/B inputs, fp32 accumulator
// Convolution is reformulated as implicit GEMM:
//   Output[M, N] = im2col(Input)[M, K] * Weight[K, N]
// where M = batch*outH*outW, K = C*R*S, N = output_channels
//
// Optimized version: 4 subgroups, 2x2 tile grid = 32x32 outputs per workgroup

layout(constant_id = 0) const uint WORKGROUP_SIZE = 128;
layout(constant_id = 1) const uint COOP_M = 16;
layout(constant_id = 2) const uint COOP_N = 16;
layout(constant_id = 3) const uint COOP_K = 16;

const uint SUBGROUP_SIZE = 32;
const uint NUM_SUBGROUPS = WORKGROUP_SIZE / SUBGROUP_SIZE;  // 4

const uint TILES_M = 2;
const uint TILES_N = 2;
const uint WG_M = TILES_M * COOP_M;  // 32
const uint WG_N = TILES_N * COOP_N;  // 32

layout(local_size_x_id = 0) in;

layout(push_constant) uniform PushConstants {
    uint N;         // Batch size
    uint C;         // Input channels
    uint H;         // Input height
    uint W;         // Input width
    uint K;         // Output channels
    uint R;         // Kernel height
    uint S;         // Kernel width
    uint outH;      // Output height
    uint outW;      // Output width
    uint padH;      // Padding height
    uint padW;      // Padding width
    uint strideH;   // Stride height
    uint strideW;   // Stride width
    uint dilationH; // Dilation height
    uint dilationW; // Dilation width
    uint groups;    // Number of groups
    uint hasBias;   // 1 if bias present, 0 otherwise
} params;

layout(binding = 0) readonly buffer Input { float input_data[]; };
layout(binding = 1) readonly buffer Weight { float weight_data[]; };
layout(binding = 2) buffer Output { float output_data[]; };
layout(binding = 3) readonly buffer Bias { float bias_data[]; };

shared float16_t smem_A[WG_M * COOP_K];
shared float16_t smem_B[COOP_K * WG_N];
shared float smem_C[TILES_M * TILES_N * COOP_M * COOP_N];

float silu(float x) {
    return x / (1.0 + exp(-x));
}

void main() {
    uint M_total = params.N * params.outH * params.outW;
    uint channelsPerGroup = params.C / params.groups;
    uint K_total = channelsPerGroup * params.R * params.S;
    uint N_out = params.K;

    uint wgTilesN = (N_out + WG_N - 1) / WG_N;
    uint wgIdx = gl_WorkGroupID.x;
    uint wgTileN = wgIdx % wgTilesN;
    uint wgTileM = wgIdx / wgTilesN;

    uint mBaseWG = wgTileM * WG_M;
    uint nBaseWG = wgTileN * WG_N;

    uint filtersPerGroup = params.K / params.groups;
    uint group = nBaseWG / filtersPerGroup;

    uint subgroupId = gl_LocalInvocationID.x / SUBGROUP_SIZE;
    uint subgroupTileM = subgroupId / TILES_N;
    uint subgroupTileN = subgroupId % TILES_N;

    coopmat<float, gl_ScopeSubgroup, COOP_M, COOP_N, gl_MatrixUseAccumulator> acc =
        coopmat<float, gl_ScopeSubgroup, COOP_M, COOP_N, gl_MatrixUseAccumulator>(0.0);

    uint tid = gl_LocalInvocationID.x;
    uint K_steps = (K_total + COOP_K - 1) / COOP_K;

    // Precomputed strides
    uint inputStride_N = params.C * params.H * params.W;
    uint inputStride_C = params.H * params.W;
    uint weightStride_K = channelsPerGroup * params.R * params.S;

    for (uint kStep = 0; kStep < K_steps; kStep++) {
        uint kBase = kStep * COOP_K;

        // Load A tile [WG_M x COOP_K]
        for (uint i = tid; i < WG_M * COOP_K; i += WORKGROUP_SIZE) {
            uint mLocal = i / COOP_K;
            uint kLocal = i % COOP_K;
            uint mGlobal = mBaseWG + mLocal;
            uint kGlobal = kBase + kLocal;

            float val = 0.0;
            if (mGlobal < M_total && kGlobal < K_total) {
                uint ow = mGlobal % params.outW;
                uint tmp = mGlobal / params.outW;
                uint oh = tmp % params.outH;
                uint n = tmp / params.outH;

                uint s = kGlobal % params.S;
                uint tmp2 = kGlobal / params.S;
                uint r = tmp2 % params.R;
                uint c_local = tmp2 / params.R;
                uint c = group * channelsPerGroup + c_local;

                int ih = int(oh * params.strideH) - int(params.padH) + int(r * params.dilationH);
                int iw = int(ow * params.strideW) - int(params.padW) + int(s * params.dilationW);

                if (ih >= 0 && ih < int(params.H) && iw >= 0 && iw < int(params.W)) {
                    val = input_data[n * inputStride_N + c * inputStride_C + uint(ih) * params.W + uint(iw)];
                }
            }
            smem_A[mLocal * COOP_K + kLocal] = float16_t(val);
        }

        // Load B tile [COOP_K x WG_N]
        for (uint i = tid; i < COOP_K * WG_N; i += WORKGROUP_SIZE) {
            uint kLocal = i / WG_N;
            uint nLocal = i % WG_N;
            uint kGlobal = kBase + kLocal;
            uint nGlobal = nBaseWG + nLocal;

            float val = 0.0;
            if (kGlobal < K_total && nGlobal < N_out) {
                uint s = kGlobal % params.S;
                uint tmp = kGlobal / params.S;
                uint r = tmp % params.R;
                uint c_local = tmp / params.R;

                val = weight_data[nGlobal * weightStride_K + c_local * params.R * params.S + r * params.S + s];
            }
            smem_B[kLocal * WG_N + nLocal] = float16_t(val);
        }

        barrier();

        coopmat<float16_t, gl_ScopeSubgroup, COOP_M, COOP_K, gl_MatrixUseA> matA;
        coopmat<float16_t, gl_ScopeSubgroup, COOP_K, COOP_N, gl_MatrixUseB> matB;

        uint aOffset = subgroupTileM * COOP_M * COOP_K;
        coopMatLoad(matA, smem_A, aOffset, COOP_K, gl_CooperativeMatrixLayoutRowMajor);

        uint bOffset = subgroupTileN * COOP_N;
        coopMatLoad(matB, smem_B, bOffset, WG_N, gl_CooperativeMatrixLayoutRowMajor);

        acc = coopMatMulAdd(matA, matB, acc);

        barrier();
    }

    // Store to shared memory
    uint cOffset = subgroupId * COOP_M * COOP_N;
    coopMatStore(acc, smem_C, cOffset, COOP_N, gl_CooperativeMatrixLayoutRowMajor);
    barrier();

    // Apply bias + SiLU and write output
    uint outputStride_K = params.outH * params.outW;
    uint outputStride_N = params.K * outputStride_K;

    for (uint i = tid; i < TILES_M * TILES_N * COOP_M * COOP_N; i += WORKGROUP_SIZE) {
        uint subgroup = i / (COOP_M * COOP_N);
        uint localIdx = i % (COOP_M * COOP_N);
        uint mLocalTile = localIdx / COOP_N;
        uint nLocalTile = localIdx % COOP_N;

        uint subgroupTileM_ = subgroup / TILES_N;
        uint subgroupTileN_ = subgroup % TILES_N;

        uint mGlobal = mBaseWG + subgroupTileM_ * COOP_M + mLocalTile;
        uint nGlobal = nBaseWG + subgroupTileN_ * COOP_N + nLocalTile;

        if (mGlobal < M_total && nGlobal < N_out) {
            float val = smem_C[i];

            if (params.hasBias != 0u) {
                val += bias_data[nGlobal];
            }
            val = silu(val);

            uint ow = mGlobal % params.outW;
            uint tmp = mGlobal / params.outW;
            uint oh = tmp % params.outH;
            uint n = tmp / params.outH;

            output_data[n * outputStride_N + nGlobal * outputStride_K + oh * params.outW + ow] = val;
        }
    }
}