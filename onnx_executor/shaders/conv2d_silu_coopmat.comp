#version 450
#extension GL_KHR_cooperative_matrix : require
#extension GL_KHR_memory_scope_semantics : require
#extension GL_KHR_shader_subgroup_basic : require
#extension GL_EXT_shader_explicit_arithmetic_types_float16 : require

// Fused Conv2D + SiLU using cooperative matrix (tensor core) acceleration
// 32x32 Tile with FP16 Accumulation
// - WG_M=32, WG_N=32
// - 4 Subgroups (128 threads) organized in 2x2 Grid
// - Acc: FP16

layout(constant_id = 0) const uint WORKGROUP_SIZE = 128;
layout(constant_id = 1) const uint COOP_M = 16;
layout(constant_id = 2) const uint COOP_N = 16;
layout(constant_id = 3) const uint COOP_K = 16;

const uint SUBGROUP_SIZE = 32;

// Configuration for 32x32 Tile
const uint SG_GRID_M = 2; 
const uint SG_GRID_N = 2; 

const uint ACC_TILES_M = 1; 
const uint ACC_TILES_N = 1; 

const uint WG_M = SG_GRID_M * ACC_TILES_M * COOP_M;  // 32
const uint WG_N = SG_GRID_N * ACC_TILES_N * COOP_N;  // 32

// Padding to avoid bank conflicts. 
const uint B_STRIDE = WG_N + 8; // 40

layout(local_size_x_id = 0) in;

layout(push_constant) uniform PushConstants {
    uint N;         // Batch size
    uint C;         // Input channels
    uint H;         // Input height
    uint W;         // Input width
    uint K;         // Output channels (N_out)
    uint R;         // Kernel height
    uint S;         // Kernel width
    uint outH;      // Output height
    uint outW;      // Output width
    uint padH;      // Padding height
    uint padW;      // Padding width
    uint strideH;   // Stride height
    uint strideW;   // Stride width
    uint dilationH; // Dilation height
    uint dilationW; // Dilation width
    uint groups;    // Number of groups
    uint hasBias;   // 1 if bias present, 0 otherwise
} params;

layout(binding = 0) readonly buffer Input { float input_data[]; };
layout(binding = 1) readonly buffer Weight { float weight_data[]; };
layout(binding = 2) buffer Output { float output_data[]; };
layout(binding = 3) readonly buffer Bias { float bias_data[]; };

// Shared Memory
shared float16_t smem_A[WG_M * COOP_K]; // 32x16 = 512
shared float16_t smem_B[COOP_K * B_STRIDE]; // 16x40 = 640
shared float16_t smem_C[WG_M * WG_N]; // 32x32 = 1024 (FP16)

shared uint smem_n[WG_M];
shared uint smem_oh[WG_M];
shared uint smem_ow[WG_M];

float silu(float x) {
    return x / (1.0 + exp(-x));
}

void main() {
    uint M_total = params.N * params.outH * params.outW;
    uint channelsPerGroup = params.C / params.groups;
    uint K_total = channelsPerGroup * params.R * params.S;
    uint N_out = params.K;

    // Workgroup Position
    uint wgTilesN = (N_out + WG_N - 1) / WG_N;
    uint wgIdx = gl_WorkGroupID.x;
    uint wgTileN = wgIdx % wgTilesN;
    uint wgTileM = wgIdx / wgTilesN;

    uint mBaseWG = wgTileM * WG_M;
    uint nBaseWG = wgTileN * WG_N;

    uint filtersPerGroup = params.K / params.groups;
    uint group = nBaseWG / filtersPerGroup;

    uint tid = gl_LocalInvocationID.x;

    // Precompute M coordinates
    if (tid < WG_M) {
        uint mLocal = tid;
        uint mGlobal = mBaseWG + mLocal;
        if (mGlobal < M_total) {
            uint ow = mGlobal % params.outW;
            uint tmp = mGlobal / params.outW;
            uint oh = tmp % params.outH;
            uint n_batch = tmp / params.outH;
            
            smem_n[mLocal] = n_batch;
            smem_oh[mLocal] = oh;
            smem_ow[mLocal] = ow;
        }
    }
    barrier();

    uint subgroupId = tid / SUBGROUP_SIZE;
    uint sgRow = subgroupId / SG_GRID_N; 
    uint sgCol = subgroupId % SG_GRID_N; 

    // Accumulators - FP16
    coopmat<float16_t, gl_ScopeSubgroup, COOP_M, COOP_N, gl_MatrixUseAccumulator> acc = coopmat<float16_t, gl_ScopeSubgroup, COOP_M, COOP_N, gl_MatrixUseAccumulator>(float16_t(0.0));

    uint K_steps = (K_total + COOP_K - 1) / COOP_K;
    uint inputStride_N = params.C * params.H * params.W;
    uint inputStride_C = params.H * params.W;
    uint weightStride_K = K_total;

    float prefetch_A[4];
    float prefetch_B[4];

    // Prologue Load
    {
        uint kStep = 0;
        uint kBase = 0;

        for (uint i = 0; i < 4; ++i) {
            uint idx = tid + i * WORKGROUP_SIZE;
            uint mLocal = idx % WG_M;
            uint kLocal = idx / WG_M;
            uint mGlobal = mBaseWG + mLocal;
            uint kGlobal = kBase + kLocal;

            float val = 0.0;
            if (mGlobal < M_total && kGlobal < K_total) {
                uint n_batch = smem_n[mLocal];
                uint oh = smem_oh[mLocal];
                uint ow = smem_ow[mLocal];
                
                uint s = kGlobal % params.S;
                uint tmp2 = kGlobal / params.S;
                uint r = tmp2 % params.R;
                uint c_local = tmp2 / params.R;
                uint c = group * channelsPerGroup + c_local;

                int ih = int(oh * params.strideH) - int(params.padH) + int(r * params.dilationH);
                int iw = int(ow * params.strideW) - int(params.padW) + int(s * params.dilationW);

                if (ih >= 0 && ih < int(params.H) && iw >= 0 && iw < int(params.W)) {
                    val = input_data[n_batch * inputStride_N + c * inputStride_C + uint(ih) * params.W + uint(iw)];
                }
            }
            prefetch_A[i] = val;
        }

        for (uint i = 0; i < 4; ++i) {
            uint idx = tid + i * WORKGROUP_SIZE;
            uint row = idx / COOP_K; 
            uint col = idx % COOP_K; 
            uint nGlobal = nBaseWG + row;
            uint kGlobal = kBase + col;

            float val = 0.0;
            if (nGlobal < N_out && kGlobal < K_total) {
                val = weight_data[nGlobal * weightStride_K + kGlobal];
            }
            prefetch_B[i] = val;
        }
    }

    for (uint kStep = 0; kStep < K_steps; kStep++) {
        // Write PREFETCH to SMEM
        for (uint i = 0; i < 4; ++i) {
            uint idx = tid + i * WORKGROUP_SIZE;
            
            // A
            uint mLocal = idx % WG_M;
            uint kLocal = idx / WG_M;
            smem_A[mLocal * COOP_K + kLocal] = float16_t(prefetch_A[i]);
            
            // B
            uint row = idx / COOP_K; 
            uint col = idx % COOP_K; 
            smem_B[col * B_STRIDE + row] = float16_t(prefetch_B[i]);
        }
        
        barrier();

        // Compute
        coopmat<float16_t, gl_ScopeSubgroup, COOP_M, COOP_K, gl_MatrixUseA> matA;
        coopmat<float16_t, gl_ScopeSubgroup, COOP_K, COOP_N, gl_MatrixUseB> matB;

        // Tile A: tileM = sgRow (0 or 1). Offset = tileM * 16.
        uint offsetA = sgRow * COOP_M * COOP_K;
        coopMatLoad(matA, smem_A, offsetA, COOP_K, gl_CooperativeMatrixLayoutRowMajor);

        // Tile B: tileN = sgCol (0 or 1). Offset = tileN * 16.
        uint offsetB = sgCol * COOP_N;
        coopMatLoad(matB, smem_B, offsetB, B_STRIDE, gl_CooperativeMatrixLayoutRowMajor);

        acc = coopMatMulAdd(matA, matB, acc);

        // Prefetch NEXT
        if (kStep < K_steps - 1) {
            uint nextK = (kStep + 1) * COOP_K;

            // Load A
            for (uint i = 0; i < 4; ++i) {
                uint idx = tid + i * WORKGROUP_SIZE;
                uint mLocal = idx % WG_M; 
                uint kLocal = idx / WG_M; 
                uint mGlobal = mBaseWG + mLocal;
                uint kGlobal = nextK + kLocal;

                float val = 0.0;
                if (mGlobal < M_total && kGlobal < K_total) {
                    uint n_batch = smem_n[mLocal];
                    uint oh = smem_oh[mLocal];
                    uint ow = smem_ow[mLocal];
                    
                    uint s = kGlobal % params.S;
                    uint r = (kGlobal / params.S) % params.R;
                    uint c_local = (kGlobal / params.S) / params.R;
                    uint c = group * channelsPerGroup + c_local;

                    int ih = int(oh * params.strideH) - int(params.padH) + int(r * params.dilationH);
                    int iw = int(ow * params.strideW) - int(params.padW) + int(s * params.dilationW);

                    if (ih >= 0 && ih < int(params.H) && iw >= 0 && iw < int(params.W)) {
                        val = input_data[n_batch * inputStride_N + c * inputStride_C + uint(ih) * params.W + uint(iw)];
                    }
                }
                prefetch_A[i] = val;
            }

            // Load B
            for (uint i = 0; i < 4; ++i) {
                uint idx = tid + i * WORKGROUP_SIZE;
                uint row = idx / COOP_K; 
                uint col = idx % COOP_K; 
                uint nGlobal = nBaseWG + row;
                uint kGlobal = nextK + col;

                float val = 0.0;
                if (nGlobal < N_out && kGlobal < K_total) {
                    val = weight_data[nGlobal * weightStride_K + kGlobal];
                }
                prefetch_B[i] = val;
            }
        }
        
        barrier();
    }

    // Store C
    uint startM = sgRow * COOP_M;
    uint startN = sgCol * COOP_N;
    uint offsetC = startM * WG_N + startN;
    coopMatStore(acc, smem_C, offsetC, WG_N, gl_CooperativeMatrixLayoutRowMajor);
    
    barrier();

    // Store Output
    // 32x32 = 1024 elements. 128 threads -> 8 elements.
    for (uint i = 0; i < 8; ++i) {
        uint idx = tid + i * WORKGROUP_SIZE;
        uint mLocal = idx / WG_N; // 0..31
        uint nLocal = idx % WG_N; // 0..31
        
        if (mLocal < WG_M) {
            uint mGlobal = mBaseWG + mLocal;
            uint nGlobal = nBaseWG + nLocal;

            if (mGlobal < M_total && nGlobal < N_out) {
                // Read float16 from smem, convert to float for Silu
                float val = float(smem_C[idx]);
                if (params.hasBias != 0u) val += bias_data[nGlobal];
                val = silu(val);

                uint ow = mGlobal % params.outW;
                uint tmp = mGlobal / params.outW;
                uint oh = tmp % params.outH;
                uint n_batch = tmp / params.outH;

                uint outIdx = n_batch * (N_out * params.outH * params.outW) 
                            + nGlobal * (params.outH * params.outW)
                            + oh * params.outW + ow;
                
                output_data[outIdx] = val;
            }
        }
    }
}
